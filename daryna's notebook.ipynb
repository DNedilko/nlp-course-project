{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Week 36\n",
    "Familiarize yourself with the dataset card, download the dataset and explore its\n",
    "columns. Summarize data statistics (size, word count, etc.) for training and\n",
    "validation data in the languages Arabic (ar), Korean (ko) and Telugu (te).\n",
    "\n",
    "For each of the languages Arabic, Korean and Telugu, report the 5 most\n",
    "common words in the questions from the training set and their count, as well\n",
    "as their English translation. What kind of words are they?\n",
    "\n",
    "Implement a rule-based classifier that predicts whether a question is an-\n",
    "swerable or impossible, only using the document (context) and question.\n",
    "\n",
    "You\n",
    "may use machine translation as a component. Use the answerable field to\n",
    "evaluate it on the validation set. What is the performance of your classifier for\n",
    "each of the languages Arabic, Korean and Telugu?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:34:23.765103Z",
     "end_time": "2025-09-05T17:34:26.477622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "validation_set = pd.read_parquet(\"data/validation.parquet\")\n",
    "train_set = pd.read_parquet(\"data/train.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:34:26.476107Z",
     "end_time": "2025-09-05T17:34:27.304621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "strip_punctuation = lambda line: [word.strip(string.punctuation+\"؟\")for word in line.split(\" \")]\n",
    "count_words = lambda line: len(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:40:46.952514Z",
     "end_time": "2025-09-05T17:40:46.972052Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dnedi\\pycharmprojects\\nlp-course-project\\newvenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:34:27.322618Z",
     "end_time": "2025-09-05T17:34:30.598954Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:34:30.600955Z",
     "end_time": "2025-09-05T17:34:53.851978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "LANG_CODE = {\"ko\": \"kor_Hang\", \"ar\": \"arb_Arab\", \"te\": \"tel_Telu\"}\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "translators = {\n",
    "    l: pipeline(\n",
    "        \"translation\",\n",
    "        model=\"facebook/nllb-200-distilled-600M\",\n",
    "        tokenizer=\"facebook/nllb-200-distilled-600M\",\n",
    "        src_lang=LANG_CODE[l],\n",
    "        tgt_lang=\"eng_Latn\",\n",
    "        device=device\n",
    "    )\n",
    "    for l in LANG_CODE\n",
    "}\n",
    "\n",
    "def translate_top_words(counts, l):\n",
    "    translate = translators[l]\n",
    "    tx = [translate(w, max_length=64)[0][\"translation_text\"] for w in counts.index]\n",
    "    counts = counts.reset_index()\n",
    "    counts[\"translation\"] = tx\n",
    "    return counts\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:48:00.198402Z",
     "end_time": "2025-09-05T17:48:11.873736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def translate_questions(df, l):\n",
    "    translate = translators[l]\n",
    "    df[\"question_translated\"] = df[\"question\"].apply(translate)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T18:17:39.176277Z",
     "end_time": "2025-09-05T18:17:39.207758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def statistics(df, col=[\"question\", \"context\"]):\n",
    "    langs = [\"ko\", \"ar\", \"te\"]\n",
    "    df = df[df['lang'].isin(langs)].copy()\n",
    "    for c in col:\n",
    "        df[f\"{c}_stripped\"] = df[c].apply(strip_punctuation)\n",
    "        df[f\"{c}_wordcount\"] = df[f\"{c}_stripped\"].apply(count_words)\n",
    "\n",
    "    print(\n",
    "        df.groupby([\"lang\", \"answerable\"]).agg(\n",
    "            question_wordcount_mean=(\"question_wordcount\", \"mean\"),\n",
    "            context_wordcount_mean=(\"context_wordcount\", \"mean\"),\n",
    "            question_wordcount_sum=(\"question_wordcount\", \"sum\"),\n",
    "            context_wordcount_sum=(\"context_wordcount\", \"sum\"),\n",
    "            question_wordcount_max=(\"question_wordcount\", \"max\"),\n",
    "            question_wordcount_min=(\"question_wordcount\", \"min\"),\n",
    "            count=(\"question_wordcount\", \"count\")\n",
    "        ).round(0)\n",
    "    )\n",
    "\n",
    "    for l in langs:\n",
    "        lang_df = df[df[\"lang\"] == l]\n",
    "        words = list(filter(None, sum(lang_df[\"question_stripped\"].tolist(), []))) #[w for lst in lang_df[\"question_stripped\"].tolist() for w in (lst or []) if w]\n",
    "        counts = pd.Series(words).value_counts().head()\n",
    "        counts = translate_top_words(counts, l)\n",
    "        print(f\"\\nTop tokens for {l} -> en:\")\n",
    "        print(counts)\n",
    "\n",
    "        df[df[\"lang\"] == l] = translate_questions(lang_df, l)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T18:17:39.395415Z",
     "end_time": "2025-09-05T18:17:39.406515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T18:17:40.027198Z",
     "end_time": "2025-09-05T18:17:40.120689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 question_wordcount_mean  context_wordcount_mean  \\\n",
      "lang answerable                                                    \n",
      "ar   False                           8.0                   112.0   \n",
      "     True                            7.0                   103.0   \n",
      "ko   False                           5.0                   108.0   \n",
      "     True                            5.0                    95.0   \n",
      "te   False                           6.0                   112.0   \n",
      "     True                            6.0                   105.0   \n",
      "\n",
      "                 question_wordcount_sum  context_wordcount_sum  \\\n",
      "lang answerable                                                  \n",
      "ar   False                          406                   5813   \n",
      "     True                          2436                  37285   \n",
      "ko   False                           95                   2051   \n",
      "     True                          1636                  32124   \n",
      "te   False                          575                  10399   \n",
      "     True                          1804                  30623   \n",
      "\n",
      "                 question_wordcount_max  question_wordcount_min  count  \n",
      "lang answerable                                                         \n",
      "ar   False                           13                       4     52  \n",
      "     True                            18                       3    363  \n",
      "ko   False                           13                       2     19  \n",
      "     True                            10                       2    337  \n",
      "te   False                           10                       3     93  \n",
      "     True                            11                       3    291  \n",
      "\n",
      "Top tokens for ko -> en:\n",
      "  index  count      translation\n",
      "0  무엇인가     75  It's something.\n",
      "1    가장     66    It's the most\n",
      "2    언제     44            When?\n",
      "3  어디인가     29   Where are you?\n",
      "4     큰     24  It's a big one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnedi\\AppData\\Local\\Temp\\ipykernel_28832\\3648181451.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"question_translated\"] = df[\"question\"].apply(translate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top tokens for ar -> en:\n",
      "  index  count  translation\n",
      "0    من    113        Who ?\n",
      "1    في     90         In .\n",
      "2    ما     81       What ?\n",
      "3    هو     66  It 's him .\n",
      "4   متى     65       When ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnedi\\AppData\\Local\\Temp\\ipykernel_28832\\3648181451.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"question_translated\"] = df[\"question\"].apply(translate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top tokens for te -> en:\n",
      "        index  count  translation\n",
      "0           ఏ     92  There is no\n",
      "1         ఏది     76  What is it?\n",
      "2        ఎవరు     74   Who is it?\n",
      "3  భారతదేశంలో     45     In India\n",
      "4         ఎంత     40     How much\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnedi\\AppData\\Local\\Temp\\ipykernel_28832\\3648181451.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"question_translated\"] = df[\"question\"].apply(translate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 question_wordcount_mean  context_wordcount_mean  \\\n",
      "lang answerable                                                    \n",
      "ar   False                           8.0                   124.0   \n",
      "     True                            7.0                   102.0   \n",
      "ko   False                           5.0                   104.0   \n",
      "     True                            5.0                    97.0   \n",
      "te   False                           6.0                   118.0   \n",
      "     True                            6.0                    87.0   \n",
      "\n",
      "                 question_wordcount_sum  context_wordcount_sum  \\\n",
      "lang answerable                                                  \n",
      "ar   False                         1948                  31580   \n",
      "     True                         15509                 234189   \n",
      "ko   False                          325                   6574   \n",
      "     True                         11524                 228653   \n",
      "te   False                          277                   5319   \n",
      "     True                          7700                 113883   \n",
      "\n",
      "                 question_wordcount_max  question_wordcount_min  count  \n",
      "lang answerable                                                         \n",
      "ar   False                           16                       4    255  \n",
      "     True                            16                       3   2303  \n",
      "ko   False                           10                       2     63  \n",
      "     True                            15                       2   2359  \n",
      "te   False                           10                       3     45  \n",
      "     True                            12                       3   1310  \n",
      "\n",
      "Top tokens for ko -> en:\n",
      "  index  count      translation\n",
      "0    가장    527    It's the most\n",
      "1  무엇인가    497  It's something.\n",
      "2    언제    336            When?\n",
      "3     몇    234           A few.\n",
      "4  어디인가    228   Where are you?\n"
     ]
    }
   ],
   "source": [
    "val_for_stat = statistics(validation_set)\n",
    "train_for_stat = statistics(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:52:46.117474Z",
     "end_time": "2025-09-05T17:53:35.120279Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " I wanted to strip punctuation, but certain languages that we do not include in the analysis have special punctuation that has to be included in thw stripping pool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_for_stat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T17:49:02.514312Z",
     "end_time": "2025-09-05T17:49:02.546952Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_for_stat.groupby([\"lang\", \"answerable\"]).agg(\n",
    "    question_wordcount_mean=(\"question_wordcount\", \"mean\"),\n",
    "    context_wordcount_mean=(\"context_wordcount\", \"mean\"),\n",
    "    count=(\"question_wordcount\", \"count\")\n",
    ").round(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T16:44:47.390594Z",
     "end_time": "2025-09-05T16:44:47.412783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T15:39:43.958583Z",
     "end_time": "2025-09-05T15:39:43.996835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": \"data/train.parquet\"}\n",
    ")[\"train\"]\n",
    "\n",
    "val_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"validation\": \"data/validation.parquet\"}\n",
    ")[\"validation\"]\n",
    "\n",
    "test_ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"test\": \"data/test.json\"}\n",
    ")[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T15:37:10.465065Z",
     "end_time": "2025-09-05T15:37:11.785545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-05T15:39:19.570015Z",
     "end_time": "2025-09-05T15:39:19.634876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
